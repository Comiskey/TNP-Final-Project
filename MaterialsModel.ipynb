{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mutli-Modal Material Classifier For Material Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If getting errors here, make sure you have necessary libraries installed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from autogluon.common.utils.utils import setup_outputdir\n",
    "from autogluon.core.utils.loaders import load_pkl\n",
    "from autogluon.core.utils.savers import save_pkl\n",
    "import os.path\n",
    "from sklearn.metrics import multilabel_confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import shap as shap\n",
    "import tkinter as tk\n",
    "from tkinter import ttk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "Classify the data through parsing of keywords in each material description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Std</th>\n",
       "      <th>ID</th>\n",
       "      <th>Material</th>\n",
       "      <th>Heat treatment</th>\n",
       "      <th>Su</th>\n",
       "      <th>Sy</th>\n",
       "      <th>A5</th>\n",
       "      <th>Bhn</th>\n",
       "      <th>E</th>\n",
       "      <th>G</th>\n",
       "      <th>mu</th>\n",
       "      <th>Ro</th>\n",
       "      <th>pH</th>\n",
       "      <th>Desc</th>\n",
       "      <th>HV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANSI</td>\n",
       "      <td>D8894772B88F495093C43AF905AB6373</td>\n",
       "      <td>Steel SAE 1015</td>\n",
       "      <td>as-rolled</td>\n",
       "      <td>421</td>\n",
       "      <td>314</td>\n",
       "      <td>39.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>207000</td>\n",
       "      <td>79000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>7860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANSI</td>\n",
       "      <td>05982AC66F064F9EBC709E7A4164613A</td>\n",
       "      <td>Steel SAE 1015</td>\n",
       "      <td>normalized</td>\n",
       "      <td>424</td>\n",
       "      <td>324</td>\n",
       "      <td>37.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>207000</td>\n",
       "      <td>79000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>7860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANSI</td>\n",
       "      <td>356D6E63FF9A49A3AB23BF66BAC85DC3</td>\n",
       "      <td>Steel SAE 1015</td>\n",
       "      <td>annealed</td>\n",
       "      <td>386</td>\n",
       "      <td>284</td>\n",
       "      <td>37.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>207000</td>\n",
       "      <td>79000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>7860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANSI</td>\n",
       "      <td>1C758F8714AC4E0D9BD8D8AE1625AECD</td>\n",
       "      <td>Steel SAE 1020</td>\n",
       "      <td>as-rolled</td>\n",
       "      <td>448</td>\n",
       "      <td>331</td>\n",
       "      <td>36.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>207000</td>\n",
       "      <td>79000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>7860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANSI</td>\n",
       "      <td>DCE10036FC1946FC8C9108D598D116AD</td>\n",
       "      <td>Steel SAE 1020</td>\n",
       "      <td>normalized</td>\n",
       "      <td>441</td>\n",
       "      <td>346</td>\n",
       "      <td>35.8</td>\n",
       "      <td>131.0</td>\n",
       "      <td>207000</td>\n",
       "      <td>79000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>7860</td>\n",
       "      <td>550.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>JIS</td>\n",
       "      <td>512A80EC21EA416BA2725B38BA8096EF</td>\n",
       "      <td>Nodular cast iron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>600</td>\n",
       "      <td>370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>169000</td>\n",
       "      <td>70000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>7160</td>\n",
       "      <td>480.0</td>\n",
       "      <td>Nodular cast iron</td>\n",
       "      <td>210.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>JIS</td>\n",
       "      <td>38526441BA8741CA979DBF870D0B8A9B</td>\n",
       "      <td>Nodular cast iron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700</td>\n",
       "      <td>420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>169000</td>\n",
       "      <td>70000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>7160</td>\n",
       "      <td>560.0</td>\n",
       "      <td>Nodular cast iron</td>\n",
       "      <td>230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>JIS</td>\n",
       "      <td>CAC03D7EB1AA45E68EFF92A2EF4C3D9B</td>\n",
       "      <td>Nodular cast iron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>800</td>\n",
       "      <td>480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>169000</td>\n",
       "      <td>70000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>7160</td>\n",
       "      <td>600.0</td>\n",
       "      <td>Nodular cast iron</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>JIS</td>\n",
       "      <td>45C82A36EC644F8BB6170A99ED819B62</td>\n",
       "      <td>Malleable cast iron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400</td>\n",
       "      <td>180</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160000</td>\n",
       "      <td>64000</td>\n",
       "      <td>0.27</td>\n",
       "      <td>7160</td>\n",
       "      <td>300.0</td>\n",
       "      <td>Malleable cast iron</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>JIS</td>\n",
       "      <td>BC74F870412F4DDBADDEF1063C1C079F</td>\n",
       "      <td>Malleable cast iron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500</td>\n",
       "      <td>260</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160000</td>\n",
       "      <td>64000</td>\n",
       "      <td>0.27</td>\n",
       "      <td>7160</td>\n",
       "      <td>370.0</td>\n",
       "      <td>Malleable cast iron</td>\n",
       "      <td>230.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1552 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Std                                ID             Material  \\\n",
       "0     ANSI  D8894772B88F495093C43AF905AB6373       Steel SAE 1015   \n",
       "1     ANSI  05982AC66F064F9EBC709E7A4164613A       Steel SAE 1015   \n",
       "2     ANSI  356D6E63FF9A49A3AB23BF66BAC85DC3       Steel SAE 1015   \n",
       "3     ANSI  1C758F8714AC4E0D9BD8D8AE1625AECD       Steel SAE 1020   \n",
       "4     ANSI  DCE10036FC1946FC8C9108D598D116AD       Steel SAE 1020   \n",
       "...    ...                               ...                  ...   \n",
       "1547   JIS  512A80EC21EA416BA2725B38BA8096EF    Nodular cast iron   \n",
       "1548   JIS  38526441BA8741CA979DBF870D0B8A9B    Nodular cast iron   \n",
       "1549   JIS  CAC03D7EB1AA45E68EFF92A2EF4C3D9B    Nodular cast iron   \n",
       "1550   JIS  45C82A36EC644F8BB6170A99ED819B62  Malleable cast iron   \n",
       "1551   JIS  BC74F870412F4DDBADDEF1063C1C079F  Malleable cast iron   \n",
       "\n",
       "     Heat treatment   Su   Sy    A5    Bhn       E      G    mu    Ro     pH  \\\n",
       "0         as-rolled  421  314  39.0  126.0  207000  79000  0.30  7860    NaN   \n",
       "1        normalized  424  324  37.0  121.0  207000  79000  0.30  7860    NaN   \n",
       "2          annealed  386  284  37.0  111.0  207000  79000  0.30  7860    NaN   \n",
       "3         as-rolled  448  331  36.0  143.0  207000  79000  0.30  7860    NaN   \n",
       "4        normalized  441  346  35.8  131.0  207000  79000  0.30  7860  550.0   \n",
       "...             ...  ...  ...   ...    ...     ...    ...   ...   ...    ...   \n",
       "1547            NaN  600  370   NaN    NaN  169000  70000  0.20  7160  480.0   \n",
       "1548            NaN  700  420   NaN    NaN  169000  70000  0.20  7160  560.0   \n",
       "1549            NaN  800  480   NaN    NaN  169000  70000  0.20  7160  600.0   \n",
       "1550            NaN  400  180   4.0    NaN  160000  64000  0.27  7160  300.0   \n",
       "1551            NaN  500  260   4.0    NaN  160000  64000  0.27  7160  370.0   \n",
       "\n",
       "                     Desc     HV  \n",
       "0                     NaN    NaN  \n",
       "1                     NaN    NaN  \n",
       "2                     NaN    NaN  \n",
       "3                     NaN    NaN  \n",
       "4                     NaN    NaN  \n",
       "...                   ...    ...  \n",
       "1547    Nodular cast iron  210.0  \n",
       "1548    Nodular cast iron  230.0  \n",
       "1549    Nodular cast iron  240.0  \n",
       "1550  Malleable cast iron  220.0  \n",
       "1551  Malleable cast iron  230.0  \n",
       "\n",
       "[1552 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Material</th>\n",
       "      <th>Su</th>\n",
       "      <th>Sy</th>\n",
       "      <th>E</th>\n",
       "      <th>G</th>\n",
       "      <th>mu</th>\n",
       "      <th>Ro</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Steel SAE 1015</td>\n",
       "      <td>421</td>\n",
       "      <td>314</td>\n",
       "      <td>207000</td>\n",
       "      <td>79000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>7860</td>\n",
       "      <td>Steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Steel SAE 1015</td>\n",
       "      <td>424</td>\n",
       "      <td>324</td>\n",
       "      <td>207000</td>\n",
       "      <td>79000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>7860</td>\n",
       "      <td>Steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Steel SAE 1015</td>\n",
       "      <td>386</td>\n",
       "      <td>284</td>\n",
       "      <td>207000</td>\n",
       "      <td>79000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>7860</td>\n",
       "      <td>Steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Steel SAE 1020</td>\n",
       "      <td>448</td>\n",
       "      <td>331</td>\n",
       "      <td>207000</td>\n",
       "      <td>79000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>7860</td>\n",
       "      <td>Steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Steel SAE 1020</td>\n",
       "      <td>441</td>\n",
       "      <td>346</td>\n",
       "      <td>207000</td>\n",
       "      <td>79000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>7860</td>\n",
       "      <td>Steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>Nodular cast iron</td>\n",
       "      <td>600</td>\n",
       "      <td>370</td>\n",
       "      <td>169000</td>\n",
       "      <td>70000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>7160</td>\n",
       "      <td>Iron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>Nodular cast iron</td>\n",
       "      <td>700</td>\n",
       "      <td>420</td>\n",
       "      <td>169000</td>\n",
       "      <td>70000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>7160</td>\n",
       "      <td>Iron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>Nodular cast iron</td>\n",
       "      <td>800</td>\n",
       "      <td>480</td>\n",
       "      <td>169000</td>\n",
       "      <td>70000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>7160</td>\n",
       "      <td>Iron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>Malleable cast iron</td>\n",
       "      <td>400</td>\n",
       "      <td>180</td>\n",
       "      <td>160000</td>\n",
       "      <td>64000</td>\n",
       "      <td>0.27</td>\n",
       "      <td>7160</td>\n",
       "      <td>Iron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>Malleable cast iron</td>\n",
       "      <td>500</td>\n",
       "      <td>260</td>\n",
       "      <td>160000</td>\n",
       "      <td>64000</td>\n",
       "      <td>0.27</td>\n",
       "      <td>7160</td>\n",
       "      <td>Iron</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1552 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Material   Su   Sy       E      G    mu    Ro  Label\n",
       "0          Steel SAE 1015  421  314  207000  79000  0.30  7860  Steel\n",
       "1          Steel SAE 1015  424  324  207000  79000  0.30  7860  Steel\n",
       "2          Steel SAE 1015  386  284  207000  79000  0.30  7860  Steel\n",
       "3          Steel SAE 1020  448  331  207000  79000  0.30  7860  Steel\n",
       "4          Steel SAE 1020  441  346  207000  79000  0.30  7860  Steel\n",
       "...                   ...  ...  ...     ...    ...   ...   ...    ...\n",
       "1547    Nodular cast iron  600  370  169000  70000  0.20  7160   Iron\n",
       "1548    Nodular cast iron  700  420  169000  70000  0.20  7160   Iron\n",
       "1549    Nodular cast iron  800  480  169000  70000  0.20  7160   Iron\n",
       "1550  Malleable cast iron  400  180  160000  64000  0.27  7160   Iron\n",
       "1551  Malleable cast iron  500  260  160000  64000  0.27  7160   Iron\n",
       "\n",
       "[1552 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Preparation\n",
    "# Rename for your device as necessary\n",
    "filename = \"Data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "display(df)\n",
    "\n",
    "def parse_and_compare(input_string, keywords):\n",
    "    # Normalize input string and tokenize\n",
    "    words = set(input_string.lower().split())\n",
    "    # Check for intersection with each keyword group\n",
    "    for label, key_group in keywords.items():\n",
    "        if words.intersection(key_group):\n",
    "            return label\n",
    "    return 'Unspecified'\n",
    "\n",
    "# Keywords dictionary\n",
    "keywords = {\n",
    "    'Steel': {'steel', 'sae', 'en', 'din', 'bs', 'csn', 'nf', 'jis'},\n",
    "    'Aluminum': {'aluminum'},\n",
    "    'Iron': {'iron', 'grey', 'cast', 'nodular'},\n",
    "    'Brass': {'brass', 'red', 'semi-red', 'yellow'},\n",
    "    'Copper': {'copper', 'high-copper'},\n",
    "    'Bronze': {'bronze', 'phosphor', 'tin', 'lead', 'silicon', 'manganese'},\n",
    "    'Magnesium': {'magnesium'}\n",
    "}\n",
    "\n",
    "# Apply the function to the entire 'Material' column\n",
    "df['Label'] = df['Material'].apply(lambda x: parse_and_compare(x, keywords))\n",
    "\n",
    "df['Sy'] = df['Sy'].str.replace(' max', '').astype(int)\n",
    "\n",
    "# Drop the unused columns\n",
    "df.drop(['Std','ID', 'Heat treatment', 'Desc','A5','Bhn','pH','Desc','HV'], axis=1, inplace=True)\n",
    "\n",
    "# Add the 'Use' column  ---> Broad material selection, not massively applicable\n",
    "# df['Use'] = (\n",
    "#     (df['Su'].between(336, 505)) &\n",
    "#     (df['Sy'].between(251, 376)) &\n",
    "#     (df['E'].between(165000, 248000)) &\n",
    "#     (df['G'].between(63000, 94000)) &\n",
    "#     (df['mu'].between(0.24, 0.36)) &\n",
    "#     (df['Ro'].between(6200, 9400))\n",
    "# ).map({True: 'True', False: 'False'})\n",
    "\n",
    "# Insert the 'Use' column at the second position\n",
    "# df.insert(7, 'Use', df.pop('Use'))\n",
    "\n",
    "# Write the updated data to a new file\n",
    "df.to_csv('BigMaterialGroups.csv', index=False)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split and Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(filename, target, onehot = False, scaling = True):\n",
    "    \"\"\"Apply one-hot encoding and scaling to the dataset as necessary\"\"\"\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.drop(['Material'], axis=1) # MAY REMOVE IN FURTURE ITERATIONS\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.drop(target, axis=1)\n",
    "    Y = df[target]\n",
    "    if onehot:\n",
    "        X = OH_encode(X)\n",
    "\n",
    "    # Filter out outliers (confirm if this work with onehot encoding)\n",
    "    for col in df.columns:\n",
    "        # Skip the target column\n",
    "        if col == target:\n",
    "            continue\n",
    "        upper_limit = df[col].quantile(0.99)\n",
    "        df = df[df[col] <= upper_limit]\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
    "\n",
    "\n",
    "    if scaling:\n",
    "        # Initialize StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # Fit the scaler on the training data and transform both training and testing data\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Reconstruct the DataFrame from scaled data\n",
    "        train_data = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "        train_data = pd.concat([train_data, Y_train.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        test_data = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "        test_data['target'] = Y_test.reset_index(drop=True)\n",
    "    else: # Simply use the unscaled data\n",
    "        train_data = pd.concat([X_train.reset_index(drop=True), Y_train.reset_index(drop=True)], axis=1)\n",
    "        test_data = pd.concat([X_test.reset_index(drop=True), Y_test.reset_index(drop=True)], axis=1)\n",
    "        test_data.rename(columns={target: 'target'}, inplace=True)\n",
    "        \n",
    "    return train_data, test_data\n",
    "\n",
    "def OH_encode(data):\n",
    "    data=data.copy()\n",
    "    #One-hot encode the materials\n",
    "    data.loc[:, \"Material\"]=pd.Categorical(data[\"Material\"], categories=[\"Steel\", \"Aluminum\", \"Titanium\"])\n",
    "    mats_oh=pd.get_dummies(data[\"Material\"], prefix=\"Material=\", prefix_sep=\"\")\n",
    "    data.drop([\"Material\"], axis=1, inplace=True)\n",
    "    data=pd.concat([mats_oh, data], axis=1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Metrics Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printScores(y_val, class_pred, average='weighted'): #proba):\n",
    "    f1 = f1_score(y_val, class_pred, average='weighted')\n",
    "    precision = precision_score(y_val, class_pred, average='weighted')\n",
    "    recall = recall_score(y_val, class_pred, average='weighted')\n",
    "    accuracy = accuracy_score(y_val, class_pred)\n",
    "    #auc = roc_auc_score(y_val, proba)\n",
    "    print(multilabel_confusion_matrix(y_val, class_pred, labels=[0, 1]))\n",
    "    print(\"F1: \" + str(f1))\n",
    "    print(\"Precision: \" + str(precision))\n",
    "    print(\"Recall: \" + str(recall))\n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "    #print(\"AUC: \" + str(auc))\n",
    "\n",
    "def returnscores_xv(Yval, predictions):\n",
    "    score = np.zeros(4)\n",
    "    score[0] = accuracy_score(Yval, predictions)\n",
    "    score[1] = recall_score(Yval, predictions, average='macro', zero_division=0)\n",
    "    score[2] = precision_score(Yval, predictions, average='macro', zero_division=0)\n",
    "    score[3] = f1_score(Yval, predictions, average='macro')\n",
    "    return score\n",
    "\n",
    "def printscores_xv(scores):\n",
    "    print(\"Accuracy: \" + str(100 * scores[0]) + \"%\")\n",
    "    print(\"Recall: \" + str(100 * scores[1]) + \"%\")\n",
    "    print(\"Precision: \" + str(100 * scores[2]) + \"%\")\n",
    "    print(\"F1: \" + str(100 * scores[3]) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AutoML**\n",
    "\n",
    "Set-up Multilabel Predictor (Autogluon only handles one by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilabelPredictor():\n",
    "    \"\"\" Tabular Predictor for predicting multiple columns in table.\n",
    "        Creates multiple TabularPredictor objects which you can also use individually.\n",
    "        You can access the TabularPredictor for a particular label via: `multilabel_predictor.get_predictor(label_i)`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        labels : List[str]\n",
    "            The ith element of this list is the column (i.e. `label`) predicted by the ith TabularPredictor stored in this object.\n",
    "        path : str, default = None\n",
    "            Path to directory where models and intermediate outputs should be saved.\n",
    "            If unspecified, a time-stamped folder called \"AutogluonModels/ag-[TIMESTAMP]\" will be created in the working directory to store all models.\n",
    "            Note: To call `fit()` twice and save all results of each fit, you must specify different `path` locations or don't specify `path` at all.\n",
    "            Otherwise files from first `fit()` will be overwritten by second `fit()`.\n",
    "            Caution: when predicting many labels, this directory may grow large as it needs to store many TabularPredictors.\n",
    "        problem_types : List[str], default = None\n",
    "            The ith element is the `problem_type` for the ith TabularPredictor stored in this object.\n",
    "        eval_metrics : List[str], default = None\n",
    "            The ith element is the `eval_metric` for the ith TabularPredictor stored in this object.\n",
    "        consider_labels_correlation : bool, default = True\n",
    "            Whether the predictions of multiple labels should account for label correlations or predict each label independently of the others.\n",
    "            If True, the ordering of `labels` may affect resulting accuracy as each label is predicted conditional on the previous labels appearing earlier in this list (i.e. in an auto-regressive fashion).\n",
    "            Set to False if during inference you may want to individually use just the ith TabularPredictor without predicting all the other labels.\n",
    "        kwargs :\n",
    "            Arguments passed into the initialization of each TabularPredictor.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    multi_predictor_file = 'multilabel_predictor.pkl'\n",
    "\n",
    "    def __init__(self, labels, path=None, problem_types=None, eval_metrics=None, consider_labels_correlation=True, **kwargs):\n",
    "        if len(labels) < 2:\n",
    "            raise ValueError(\"MultilabelPredictor is only intended for predicting MULTIPLE labels (columns), use TabularPredictor for predicting one label (column).\")\n",
    "        if (problem_types is not None) and (len(problem_types) != len(labels)):\n",
    "            raise ValueError(\"If provided, `problem_types` must have same length as `labels`\")\n",
    "        if (eval_metrics is not None) and (len(eval_metrics) != len(labels)):\n",
    "            raise ValueError(\"If provided, `eval_metrics` must have same length as `labels`\")\n",
    "        self.path = setup_outputdir(path, warn_if_exist=False)\n",
    "        self.labels = labels\n",
    "        self.consider_labels_correlation = consider_labels_correlation\n",
    "        self.predictors = {}  # key = label, value = TabularPredictor or str path to the TabularPredictor for this label\n",
    "        if eval_metrics is None:\n",
    "            self.eval_metrics = {}\n",
    "        else:\n",
    "            self.eval_metrics = {labels[i] : eval_metrics[i] for i in range(len(labels))}\n",
    "        problem_type = None\n",
    "        eval_metric = None\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            path_i = self.path + \"Predictor_\" + label\n",
    "            if problem_types is not None:\n",
    "                problem_type = problem_types[i]\n",
    "            if eval_metrics is not None:\n",
    "                eval_metric = eval_metrics[i]\n",
    "            self.predictors[label] = TabularPredictor(label=label, problem_type=problem_type, eval_metric=eval_metric, path=path_i, **kwargs)\n",
    "\n",
    "    def fit(self, train_data, tuning_data=None, **kwargs):\n",
    "        \"\"\" Fits a separate TabularPredictor to predict each of the labels.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            train_data, tuning_data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                See documentation for `TabularPredictor.fit()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `fit()` call for each TabularPredictor.\n",
    "        \"\"\"\n",
    "        if isinstance(train_data, str):\n",
    "            train_data = TabularDataset(train_data)\n",
    "        if tuning_data is not None and isinstance(tuning_data, str):\n",
    "            tuning_data = TabularDataset(tuning_data)\n",
    "        train_data_og = train_data.copy()\n",
    "        if tuning_data is not None:\n",
    "            tuning_data_og = tuning_data.copy()\n",
    "        else:\n",
    "            tuning_data_og = None\n",
    "        save_metrics = len(self.eval_metrics) == 0\n",
    "        for i in range(len(self.labels)):\n",
    "            label = self.labels[i]\n",
    "            predictor = self.get_predictor(label)\n",
    "            if not self.consider_labels_correlation:\n",
    "                labels_to_drop = [l for l in self.labels if l != label]\n",
    "            else:\n",
    "                labels_to_drop = [self.labels[j] for j in range(i+1, len(self.labels))]\n",
    "            train_data = train_data_og.drop(labels_to_drop, axis=1)\n",
    "            if tuning_data is not None:\n",
    "                tuning_data = tuning_data_og.drop(labels_to_drop, axis=1)\n",
    "            print(f\"Fitting TabularPredictor for label: {label} ...\")\n",
    "            predictor.fit(train_data=train_data, tuning_data=tuning_data, **kwargs)\n",
    "            self.predictors[label] = predictor.path\n",
    "            if save_metrics:\n",
    "                self.eval_metrics[label] = predictor.eval_metric\n",
    "        self.save()\n",
    "\n",
    "    def predict(self, data, **kwargs):\n",
    "        \"\"\" Returns DataFrame with label columns containing predictions for each label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to make predictions for. If label columns are present in this data, they will be ignored. See documentation for `TabularPredictor.predict()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the predict() call for each TabularPredictor.\n",
    "        \"\"\"\n",
    "        return self._predict(data, as_proba=False, **kwargs)\n",
    "\n",
    "    def predict_proba(self, data, **kwargs):\n",
    "        \"\"\" Returns dict where each key is a label and the corresponding value is the `predict_proba()` output for just that label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to make predictions for. See documentation for `TabularPredictor.predict()` and `TabularPredictor.predict_proba()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `predict_proba()` call for each TabularPredictor (also passed into a `predict()` call).\n",
    "        \"\"\"\n",
    "        return self._predict(data, as_proba=True, **kwargs)\n",
    "\n",
    "    def evaluate(self, data, **kwargs):\n",
    "        \"\"\" Returns dict where each key is a label and the corresponding value is the `evaluate()` output for just that label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to evalate predictions of all labels for, must contain all labels as columns. See documentation for `TabularPredictor.evaluate()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `evaluate()` call for each TabularPredictor (also passed into the `predict()` call).\n",
    "        \"\"\"\n",
    "        data = self._get_data(data)\n",
    "        eval_dict = {}\n",
    "        for label in self.labels:\n",
    "            print(f\"Evaluating TabularPredictor for label: {label} ...\")\n",
    "            predictor = self.get_predictor(label)\n",
    "            eval_dict[label] = predictor.evaluate(data, **kwargs)\n",
    "            if self.consider_labels_correlation:\n",
    "                data[label] = predictor.predict(data, **kwargs)\n",
    "        return eval_dict\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\" Save MultilabelPredictor to disk. \"\"\"\n",
    "        for label in self.labels:\n",
    "            if not isinstance(self.predictors[label], str):\n",
    "                self.predictors[label] = self.predictors[label].path\n",
    "        save_pkl.save(path=self.path+self.multi_predictor_file, object=self)\n",
    "        print(f\"MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('{self.path}')\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\" Load MultilabelPredictor from disk `path` previously specified when creating this MultilabelPredictor. \"\"\"\n",
    "        path = os.path.expanduser(path)\n",
    "        if path[-1] != os.path.sep:\n",
    "            path = path + os.path.sep\n",
    "        return load_pkl.load(path=path+cls.multi_predictor_file)\n",
    "\n",
    "    def get_predictor(self, label):\n",
    "        \"\"\" Returns TabularPredictor which is used to predict this label. \"\"\"\n",
    "        predictor = self.predictors[label]\n",
    "        if isinstance(predictor, str):\n",
    "            return TabularPredictor.load(path=predictor)\n",
    "        return predictor\n",
    "\n",
    "    def _get_data(self, data):\n",
    "        if isinstance(data, str):\n",
    "            return TabularDataset(data)\n",
    "        return data.copy()\n",
    "\n",
    "    def _predict(self, data, as_proba=False, **kwargs):\n",
    "        data = self._get_data(data)\n",
    "        if as_proba:\n",
    "            predproba_dict = {}\n",
    "        for label in self.labels:\n",
    "#             print(f\"Predicting with TabularPredictor for label: {label} ...\")\n",
    "            predictor = self.get_predictor(label)\n",
    "            if as_proba:\n",
    "                predproba_dict[label] = predictor.predict_proba(data, as_multiclass=True, **kwargs)\n",
    "            data[label] = predictor.predict(data, **kwargs)\n",
    "        if not as_proba:\n",
    "            return data[self.labels]\n",
    "        else:\n",
    "            return predproba_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "39/39 [==============================] - 1s 3ms/step - loss: 1.2202 - accuracy: 0.7969\n",
      "Epoch 2/50\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.4494 - accuracy: 0.8703\n",
      "Epoch 3/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3556 - accuracy: 0.8791\n",
      "Epoch 4/50\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.3280 - accuracy: 0.8928\n",
      "Epoch 5/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3112 - accuracy: 0.8993\n",
      "Epoch 6/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3013 - accuracy: 0.9001\n",
      "Epoch 7/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2882 - accuracy: 0.9025\n",
      "Epoch 8/50\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.2838 - accuracy: 0.9009\n",
      "Epoch 9/50\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2702 - accuracy: 0.9017\n",
      "Epoch 10/50\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2617 - accuracy: 0.9025\n",
      "Epoch 11/50\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.2562 - accuracy: 0.9025\n",
      "Epoch 12/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.9073\n",
      "Epoch 13/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2381 - accuracy: 0.9130\n",
      "Epoch 14/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2319 - accuracy: 0.9106\n",
      "Epoch 15/50\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.2239 - accuracy: 0.9194\n",
      "Epoch 16/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2139 - accuracy: 0.9234\n",
      "Epoch 17/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2075 - accuracy: 0.9355\n",
      "Epoch 18/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2020 - accuracy: 0.9331\n",
      "Epoch 19/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1977 - accuracy: 0.9339\n",
      "Epoch 20/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1898 - accuracy: 0.9380\n",
      "Epoch 21/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1863 - accuracy: 0.9404\n",
      "Epoch 22/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1816 - accuracy: 0.9404\n",
      "Epoch 23/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1869 - accuracy: 0.9388\n",
      "Epoch 24/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1745 - accuracy: 0.9452\n",
      "Epoch 25/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1733 - accuracy: 0.9428\n",
      "Epoch 26/50\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1728 - accuracy: 0.9452\n",
      "Epoch 27/50\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.1673 - accuracy: 0.9468\n",
      "Epoch 28/50\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1675 - accuracy: 0.9444\n",
      "Epoch 29/50\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1630 - accuracy: 0.9404\n",
      "Epoch 30/50\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1630 - accuracy: 0.9420\n",
      "Epoch 31/50\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1586 - accuracy: 0.9468\n",
      "Epoch 32/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1585 - accuracy: 0.9508\n",
      "Epoch 33/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1566 - accuracy: 0.9476\n",
      "Epoch 34/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1560 - accuracy: 0.9444\n",
      "Epoch 35/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1562 - accuracy: 0.9492\n",
      "Epoch 36/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1553 - accuracy: 0.9476\n",
      "Epoch 37/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1543 - accuracy: 0.9517\n",
      "Epoch 38/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1534 - accuracy: 0.9492\n",
      "Epoch 39/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1500 - accuracy: 0.9508\n",
      "Epoch 40/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1451 - accuracy: 0.9484\n",
      "Epoch 41/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1429 - accuracy: 0.9492\n",
      "Epoch 42/50\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1514 - accuracy: 0.9517\n",
      "Epoch 43/50\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.1484 - accuracy: 0.9468\n",
      "Epoch 44/50\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1462 - accuracy: 0.9500\n",
      "Epoch 45/50\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.1404 - accuracy: 0.9500\n",
      "Epoch 46/50\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1416 - accuracy: 0.9541\n",
      "Epoch 47/50\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1452 - accuracy: 0.9517\n",
      "Epoch 48/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1438 - accuracy: 0.9508\n",
      "Epoch 49/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1392 - accuracy: 0.9508\n",
      "Epoch 50/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1378 - accuracy: 0.9492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231201_032229\\\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231201_032229\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.2\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Disk Space Avail:   32.11 GB / 510.96 GB (6.3%)\n",
      "Train Data Rows:    1241\n",
      "Train Data Columns: 6\n",
      "Label Column: Label\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\t8 unique label values:  ['Steel', 'Copper', 'Aluminum', 'Iron', 'Brass', 'Bronze', 'Magnesium', 'Unspecified']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Train Data Class Count: 8\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3512.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.06 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 6 | ['Su', 'Sy', 'E', 'G', 'mu', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 6 | ['Su', 'Sy', 'E', 'G', 'mu', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t6 features in original data used to generate 6 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 992, Val Rows: 249\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.9438\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.9558\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.9398\t = Validation score   (accuracy)\n",
      "\t1.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.9518\t = Validation score   (accuracy)\n",
      "\t1.31s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.9679\t = Validation score   (accuracy)\n",
      "\t1.67s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9478\t = Validation score   (accuracy)\n",
      "\t1.19s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9518\t = Validation score   (accuracy)\n",
      "\t1.13s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.9639\t = Validation score   (accuracy)\n",
      "\t4.61s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.9478\t = Validation score   (accuracy)\n",
      "\t1.27s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.9478\t = Validation score   (accuracy)\n",
      "\t1.04s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.9639\t = Validation score   (accuracy)\n",
      "\t1.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9558\t = Validation score   (accuracy)\n",
      "\t8.28s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.9478\t = Validation score   (accuracy)\n",
      "\t3.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.9679\t = Validation score   (accuracy)\n",
      "\t1.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 29.73s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231201_032229\\\")\n"
     ]
    }
   ],
   "source": [
    "# Get training and test data\n",
    "target = 'Label'\n",
    "train_data, test_data = process_dataset('BigMaterialGroups.csv', target, onehot = False, scaling = True) # Note that this process renames \"Label\" to \"target\"\n",
    "\n",
    "# Separating features and target for traditional ML models\n",
    "X_train = train_data.drop(target, axis=1)\n",
    "Y_train = train_data[target]\n",
    "\n",
    "# Train Logistic Regression model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, Y_train)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, Y_train)\n",
    "\n",
    "# Train FFNN model\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.round(tf.nn.softmax(y_pred))\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "Y_train_encoded = label_encoder.fit_transform(Y_train)\n",
    "\n",
    "ffnn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='softmax') # 8 classes, adjust if number of labels changes\n",
    "])\n",
    "ffnn_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "ffnn_model.fit(X_train, Y_train_encoded, epochs=50, batch_size=32)\n",
    "\n",
    "# Train autoML model\n",
    "autoML_model = TabularPredictor(label=target).fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Evaluation:\n",
      "[[[311   0]\n",
      "  [  0   0]]\n",
      "\n",
      " [[311   0]\n",
      "  [  0   0]]]\n",
      "F1: 0.8863534395902106\n",
      "Precision: 0.891667121015348\n",
      "Recall: 0.9035369774919614\n",
      "Accuracy: 0.9035369774919614\n",
      "\n",
      "Random Forest Evaluation:\n",
      "[[[311   0]\n",
      "  [  0   0]]\n",
      "\n",
      " [[311   0]\n",
      "  [  0   0]]]\n",
      "F1: 0.9809972016495088\n",
      "Precision: 0.9838768948093707\n",
      "Recall: 0.9807073954983923\n",
      "Accuracy: 0.9807073954983923\n",
      " 1/10 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 3ms/step\n",
      "\n",
      "FFNN Evaluation:\n",
      "[[[311   0]\n",
      "  [  0   0]]\n",
      "\n",
      " [[311   0]\n",
      "  [  0   0]]]\n",
      "F1: 0.9844925072822375\n",
      "Precision: 0.9863782519731072\n",
      "Recall: 0.9839228295819936\n",
      "Accuracy: 0.9839228295819936\n",
      "\n",
      "AutoGluon Evaluation:\n",
      "[[[311   0]\n",
      "  [  0   0]]\n",
      "\n",
      " [[311   0]\n",
      "  [  0   0]]]\n",
      "F1: 0.9799001481519675\n",
      "Precision: 0.9770473972935442\n",
      "Recall: 0.9839228295819936\n",
      "Accuracy: 0.9839228295819936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    }
   ],
   "source": [
    "# True labels\n",
    "y_test = test_data['target']\n",
    "\n",
    "# Data without labels\n",
    "test_data_nolab = test_data.drop(labels=['target'], axis=1)\n",
    "#test_data_nolab_encoded = label_encoder.transform(test_data_nolab)\n",
    "\n",
    "# Logistic Regression Evaluation\n",
    "lr_y_pred = lr_model.predict(test_data_nolab)\n",
    "print(\"Logistic Regression Evaluation:\")\n",
    "printScores(y_test, lr_y_pred, average='weighted')\n",
    "\n",
    "# Random Forest Evaluation\n",
    "rf_y_pred = rf_model.predict(test_data_nolab)\n",
    "print(\"\\nRandom Forest Evaluation:\")\n",
    "printScores(y_test, rf_y_pred, average='weighted')\n",
    "\n",
    "# FFNN Evaluation\n",
    "# Convert test data to array for TensorFlow model\n",
    "test_data_array = test_data_nolab.to_numpy()\n",
    "# Predict using FFNN model\n",
    "ffnn_y_pred = ffnn_model.predict(test_data_array)\n",
    "# Convert probabilities to class labels\n",
    "ffnn_y_pred = tf.argmax(ffnn_y_pred, axis=1).numpy()\n",
    "ffnn_y_pred = label_encoder.inverse_transform(ffnn_y_pred)\n",
    "print(\"\\nFFNN Evaluation:\")\n",
    "printScores(y_test, ffnn_y_pred, average='weighted')\n",
    "\n",
    "# AutoGluon Evaluation\n",
    "y_pred_autogluon = autoML_model.predict(test_data_nolab)\n",
    "print(\"\\nAutoGluon Evaluation:\")\n",
    "printScores(y_test, y_pred_autogluon, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Interpretation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "shap.intitjs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User Interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "# Function to collect and process inputs, then display outputs\n",
    "def submit():\n",
    "    inputs = {}\n",
    "    for var, entry in entries.items():\n",
    "        inputs[var] = entry.get()\n",
    "\n",
    "    # Simulate model prediction\n",
    "    results = plug_into_model(inputs)\n",
    "\n",
    "    # Display the results\n",
    "    display_results(results)\n",
    "\n",
    "# Mock function to simulate ML model predictions\n",
    "def plug_into_model(user_inputs, model):\n",
    "    # This is just a placeholder. Replace with your actual ML model logic.\n",
    "    materials = [\"Steel\", \"Aluminum\", \"Titanium\", \"Copper\"]\n",
    "    properties = [\"High Strength\", \"Lightweight\", \"Corrosion Resistant\", \"High Conductivity\"]\n",
    "    results = []\n",
    "    for _ in range(random.randint(1, 5)):  # Generate a random number of results\n",
    "        material = random.choice(materials)\n",
    "        property = random.choice(properties)\n",
    "        results.append(f\"{material}: {property}\")\n",
    "    return results\n",
    "\n",
    "# Function to display the results in a new window\n",
    "def display_results(results):\n",
    "    result_window = tk.Toplevel(root)\n",
    "    result_window.title(\"AutoML Materials Classifier Results\")\n",
    "\n",
    "    ttk.Label(result_window, text=\"Predicted Material Properties:\", font=(\"Arial\", 14)).pack(pady=10)\n",
    "\n",
    "    for result in results:\n",
    "        ttk.Label(result_window, text=result).pack()\n",
    "\n",
    "# GUI CREATION\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Materials Property ML Classifier\")\n",
    "\n",
    "# Define the variables you want inputs for\n",
    "# Define input variables\n",
    "kaggle_variables = [\"Young's Modulus [MPa]\", \"Shear Modulus [GPa]\", \n",
    "                    \"Yield Strength [MPa]\", \"Ultimate Tensile Strength [MPa]\", \n",
    "                    \"Poisson's Ration [/]\", \"Density [kg/m^3]\"]\n",
    "\n",
    "# Create a dictionary to hold the entry widgets\n",
    "entries = {}\n",
    "\n",
    "# Create and pack the widgets for inputs\n",
    "for var in kaggle_variables:\n",
    "    frame = ttk.Frame(root)\n",
    "    frame.pack(fill='x', padx=5, pady=5)\n",
    "\n",
    "    label = ttk.Label(frame, text=var)\n",
    "    label.pack(side='left')\n",
    "\n",
    "    entry = ttk.Entry(frame)\n",
    "    entry.pack(side='right', expand=True, fill='x')\n",
    "    entries[var] = entry\n",
    "\n",
    "# Submit button\n",
    "submit_button = ttk.Button(root, text=\"Submit\", command=submit)\n",
    "submit_button.pack(pady=10)\n",
    "\n",
    "# Start the GUI event loop\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a503a4f48930a71509f685f666416555aa62d712582ca85f12333b244ed81455"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
